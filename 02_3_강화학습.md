# 2.3 강화학습 (Reinforcement Learning)

강화학습(Reinforcement Learning, RL)은 **에이전트(Agent)**가 **환경(Environment)**과 상호작용하며, **보상(Reward)**을 기반으로 최적의 행동 정책(Policy)을 학습하는 학습 방식입니다. 지도학습처럼 정답을 주지 않고, 시행착오를 통해 **장기적 보상을 최대화**하는 행동 전략을 개발합니다.

---

## 📖 개념 구조

- **에이전트 (Agent)**: 학습 주체
- **환경 (Environment)**: 에이전트가 행동하는 세계
- **상태 (State, s)**: 현재 상황의 표현
- **행동 (Action, a)**: 에이전트가 취할 수 있는 선택
- **보상 (Reward, r)**: 행동의 결과로 받는 값
- **정책 (Policy, π)**: 상태에 따라 행동을 선택하는 전략
- **가치 함수 (Value Function, V or Q)**: 상태 또는 상태-행동 쌍의 장기적 기대 보상

> 목표: 누적 보상 \( R = \sum_{t=0}^{\infty} \gamma^t r_t \) 최대화

---

## 🧮 수학적 기반: 마르코프 결정 과정 (MDP)

강화학습은 일반적으로 MDP로 모델링됩니다.

\[
MDP = (S, A, P, R, \gamma)
\]

- \( S \): 상태 공간
- \( A \): 행동 공간
- \( P(s'|s,a) \): 상태 전이 확률
- \( R(s,a) \): 보상 함수
- \( \gamma \): 할인율 (미래 보상의 현재 가치 반영)

### 벨만 방정식 (Bellman Equation)

\[
V^\pi(s) = \mathbb{E}_\pi \left[ r_t + \gamma V^\pi(s') \right]
\]

\[
Q^\pi(s,a) = \mathbb{E}_\pi \left[ r_t + \gamma Q^\pi(s', a') \right]
\]

---

## 🤖 주요 알고리즘

### 🔹 값 기반(Value-Based)
- **Q-Learning**  
  : 상태-행동 가치 \( Q(s,a) \)를 직접 추정  
  \[
  Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s,a)]
  \]

- **Deep Q-Network (DQN)**  
  : Q-Learning + 딥러닝 (CNN, RNN 등)

### 🔹 정책 기반(Policy-Based)
- **REINFORCE**  
  : 정책을 직접 최적화  
  \[
  \nabla J(\theta) = \mathbb{E} \left[ \nabla_\theta \log \pi_\theta(a|s) R \right]
  \]

### 🔹 Actor-Critic 계열
- Actor: 정책 함수 업데이트  
- Critic: 가치 함수 평가  
- A2C, A3C, PPO 등

---

## 💡 실제 활용 사례

| 분야 | 적용 사례 |
|------|-----------|
| 게임 | AlphaGo, OpenAI Five (Dota2), Atari |
| 로보틱스 | 팔 제어, 균형 유지, 장애물 회피 |
| 자율주행 | 경로 결정, 행동 계획 |
| 금융 | 포트폴리오 최적화, 거래 전략 |
| 산업 제어 | 스마트 팩토리 자동화 |

---

## 📊 평가 방식

- 총 보상 (Cumulative Reward)
- 에피소드 수 대비 수렴 속도
- 정책 안정성
- 탐험(exploration) vs 활용(exploitation) 균형

---

## 🧠 장점과 단점

### ✅ 장점
- 명확한 지도 데이터 없이 학습 가능
- 복잡한 환경에서도 높은 성능 발휘
- 자율적 의사결정 및 전략적 사고 가능

### ⚠️ 단점
- 학습 시간 오래 걸림
- 샘플 효율성 낮음
- 환경 설계와 보상 정의가 민감함

---

> 이전: [2.2 비지도학습](02_2_비지도학습.md)
